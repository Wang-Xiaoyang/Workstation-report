{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worksation Test Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the functionality and computation ability of two workstations, referred as 'beta' and 'gamma' in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardware\n",
    "- #### Beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "CPU: Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz\n",
    "\n",
    "Memory: 64 GB\n",
    "\n",
    "Disk: \n",
    "\n",
    "    Name     Size     MountPoint\n",
    "    \n",
    "    sda      5.5T     /mnt *\n",
    "    nvme0n1  465.8GB  /\n",
    "\n",
    "RTX 2080 Ti (2)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "CPU: Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz\n",
    "\n",
    "Memory: 64 GB\n",
    "\n",
    "Disk: \n",
    "\n",
    "    Name     Size     MountPoint\n",
    "    \n",
    "    sda      5.5T     /mnt *\n",
    "    nvme0n1  465.8GB  /\n",
    "\n",
    "RTX 2080 Ti (2)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note * : sda needs to be re-mounted everytime the workstation is rebooted. Assume the mount point is /mnt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    sudo mount /dev/sda /mnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imagenet** and the **one-billion-word** datasets have been downloaded and saved in the disk 'sda'. Assume the sda is mounted to '/mnt', then the datasets could be found in '/mnt/data/'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software\n",
    "- #### Beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "Ubuntu: 18.04\n",
    "\n",
    "Nvidia driver: 415.27 (**originally came with 410.79**)\n",
    "\n",
    "Cuda: 10.0\n",
    "\n",
    "Docker version: 18.09.1\n",
    "\n",
    "Nvidia docker version: 2.0.3\n",
    "\n",
    "Container images* : nvcr.io/nvidia/tensorflow:18.12-py3 (pulled from\n",
    "                                            https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow** ) \n",
    "                                            with tensorflow: 1.12.0\n",
    "                  \n",
    "                  nvcr.io/nvidia/tensorflow:18.09-py3 (originally installed; with tensorflow 1.10.0)\n",
    "                  \n",
    "                  tensorflow/notebook (\n",
    "                                  nvcr.io/nvidia/tensorflow:18.09-py3 with jupyter notebook installed)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "Ubuntu: 18.04\n",
    "\n",
    "Nvidia driver: 415.25\n",
    "\n",
    "Cuda: 10.0\n",
    "\n",
    "Docker version: 18.09.0\n",
    "\n",
    "Nvidia docker version: 2.0.3\n",
    "\n",
    "Container images* : nvcr.io/nvidia/tensorflow:18.12-py3 (pulled from\n",
    "                                            https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow** )\n",
    "                                            with tensorflow: 1.12.0\n",
    "                  \n",
    "                  nvcr.io/nvidia/tensorflow:18.09-py3 (originally installed; with tensorflow 1.10.0)\n",
    "                  \n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: to see all the images, use the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo docker image ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note: For more Nvidia containers available, please visit https://ngc.nvidia.com/catalog/containers.\n",
    "\n",
    "Follow the pull guide and the containers will be ready to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Beta\n",
    "\n",
    "NVlink capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvidia-smi nvlink -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "GPU 0: GeForce RTX 2080 Ti (UUID: GPU-8a4b6f07-8dac-b3de-ff91-123545be42e5)\n",
    "\t Link 0, P2P is supported: true\n",
    "\t Link 0, Access to system memory supported: true\n",
    "\t Link 0, P2P atomics supported: true\n",
    "\t Link 0, System memory atomics supported: true\n",
    "\t Link 0, SLI is supported: true\n",
    "\t Link 0, Link is supported: false\n",
    "\t Link 1, P2P is supported: true\n",
    "\t Link 1, Access to system memory supported: true\n",
    "\t Link 1, P2P atomics supported: true\n",
    "\t Link 1, System memory atomics supported: true\n",
    "\t Link 1, SLI is supported: true\n",
    "\t Link 1, Link is supported: false\n",
    "GPU 1: GeForce RTX 2080 Ti (UUID: GPU-a8c359f6-3763-4bb0-d910-dba44e6b9405)\n",
    "\t Link 0, P2P is supported: true\n",
    "\t Link 0, Access to system memory supported: true\n",
    "\t Link 0, P2P atomics supported: true\n",
    "\t Link 0, System memory atomics supported: true\n",
    "\t Link 0, SLI is supported: true\n",
    "\t Link 0, Link is supported: false\n",
    "\t Link 1, P2P is supported: true\n",
    "\t Link 1, Access to system memory supported: true\n",
    "\t Link 1, P2P atomics supported: true\n",
    "\t Link 1, System memory atomics supported: true\n",
    "\t Link 1, SLI is supported: true\n",
    "\t Link 1, Link is supported: false\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peer-to-peer performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/usr/local/cuda-10.0/samples/0_Simple/simpleP2P/simpleP2P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "[./simpleP2P] - Starting...\n",
    "Checking for multiple GPUs...\n",
    "CUDA-capable device count: 2\n",
    "> GPU0 = \"GeForce RTX 2080 Ti\" IS  capable of Peer-to-Peer (P2P)\n",
    "> GPU1 = \"GeForce RTX 2080 Ti\" IS  capable of Peer-to-Peer (P2P)\n",
    "\n",
    "Checking GPU(s) for support of peer to peer memory access...\n",
    "> Peer access from GeForce RTX 2080 Ti (GPU0) -> GeForce RTX 2080 Ti (GPU1) : Yes\n",
    "> Peer access from GeForce RTX 2080 Ti (GPU1) -> GeForce RTX 2080 Ti (GPU0) : Yes\n",
    "\n",
    "Enabling peer access between GPU0 and GPU1...\n",
    "Checking GPU0 and GPU1 for UVA capabilities...\n",
    "> GeForce RTX 2080 Ti (GPU0) supports UVA: Yes\n",
    "> GeForce RTX 2080 Ti (GPU1) supports UVA: Yes\n",
    "\n",
    "Both GPUs can support UVA, enabling...\n",
    "Allocating buffers (64MB on GPU0, GPU1 and CPU Host)...\n",
    "Creating event handles...\n",
    "\n",
    "cudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 43.57GB/s\n",
    "\n",
    "Preparing host buffer and memcpy to GPU0...\n",
    "Run kernel on GPU1, taking source data from GPU0 and writing to GPU1...\n",
    "Run kernel on GPU0, taking source data from GPU1 and writing to GPU0...\n",
    "Copy data back to host from GPU0 and verify results...\n",
    "Disabling peer access...\n",
    "Shutting down...\n",
    "Test passed\n",
    "\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P2P bandwidth latency test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/usr/local/cuda-10.0/samples/1_Utilities/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\n",
    "So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.\n",
    "\n",
    "P2P Connectivity Matrix\n",
    "     D\\D     0     1\n",
    "     0\t     1     1\n",
    "     1\t     1     1\n",
    "Unidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1 \n",
    "     0 529.08   5.92 \n",
    "     1   5.94 532.30 \n",
    "Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\n",
    "   D\\D     0      1 \n",
    "     0 531.69  46.93 \n",
    "     1  46.98 531.64 \n",
    "Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1 \n",
    "     0 535.83   6.41 \n",
    "     1   6.40 535.37 \n",
    "Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1 \n",
    "     0 528.71  93.45 \n",
    "     1  93.71 533.17 \n",
    "P2P=Disabled Latency Matrix (us)\n",
    "   GPU     0      1 \n",
    "     0   1.77  12.78 \n",
    "     1  12.91   1.89 \n",
    "\n",
    "   CPU     0      1 \n",
    "     0   2.33   5.02 \n",
    "     1   5.05   2.28 \n",
    "P2P=Enabled Latency (P2P Writes) Matrix (us)\n",
    "   GPU     0      1 \n",
    "     0   1.47   1.89 \n",
    "     1   2.06   1.89 \n",
    "\n",
    "   CPU     0      1 \n",
    "     0   2.30   1.77 \n",
    "     1   1.81   2.30 \n",
    "\n",
    "NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional bandwidth with 2 2080 Ti over NVlink: ~94 GB/s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Gamma\n",
    "\n",
    "NVlink capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "GPU 0: GeForce RTX 2080 Ti (UUID: GPU-f551554c-316a-cbdf-2741-679ff11e07bc)\n",
    "\t Link 0, P2P is supported: true\n",
    "\t Link 0, Access to system memory supported: true\n",
    "\t Link 0, P2P atomics supported: true\n",
    "\t Link 0, System memory atomics supported: true\n",
    "\t Link 0, SLI is supported: true\n",
    "\t Link 0, Link is supported: false\n",
    "\t Link 1, P2P is supported: true\n",
    "\t Link 1, Access to system memory supported: true\n",
    "\t Link 1, P2P atomics supported: true\n",
    "\t Link 1, System memory atomics supported: true\n",
    "\t Link 1, SLI is supported: true\n",
    "\t Link 1, Link is supported: false\n",
    "GPU 1: GeForce RTX 2080 Ti (UUID: GPU-643a5241-5647-3676-4359-8e73f6a5b900)\n",
    "\t Link 0, P2P is supported: true\n",
    "\t Link 0, Access to system memory supported: true\n",
    "\t Link 0, P2P atomics supported: true\n",
    "\t Link 0, System memory atomics supported: true\n",
    "\t Link 0, SLI is supported: true\n",
    "\t Link 0, Link is supported: false\n",
    "\t Link 1, P2P is supported: true\n",
    "\t Link 1, Access to system memory supported: true\n",
    "\t Link 1, P2P atomics supported: true\n",
    "\t Link 1, System memory atomics supported: true\n",
    "\t Link 1, SLI is supported: true\n",
    "\t Link 1, Link is supported: false\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peer-to-Peer performance.\n",
    "\n",
    "**Note**: Gamma didn't come with Cuda 10.0 toolkit. It's in the containers but not in the host system. Thus manually installed Cuda 10.0 for P2P test and further use.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "[./simpleP2P] - Starting...\n",
    "Checking for multiple GPUs...\n",
    "CUDA-capable device count: 2\n",
    "> GPU0 = \"GeForce RTX 2080 Ti\" IS  capable of Peer-to-Peer (P2P)\n",
    "> GPU1 = \"GeForce RTX 2080 Ti\" IS  capable of Peer-to-Peer (P2P)\n",
    "\n",
    "Checking GPU(s) for support of peer to peer memory access...\n",
    "> Peer access from GeForce RTX 2080 Ti (GPU0) -> GeForce RTX 2080 Ti (GPU1) : Yes\n",
    "> Peer access from GeForce RTX 2080 Ti (GPU1) -> GeForce RTX 2080 Ti (GPU0) : Yes\n",
    "\n",
    "Enabling peer access between GPU0 and GPU1...\n",
    "Checking GPU0 and GPU1 for UVA capabilities...\n",
    "> GeForce RTX 2080 Ti (GPU0) supports UVA: Yes\n",
    "> GeForce RTX 2080 Ti (GPU1) supports UVA: Yes\n",
    "\n",
    "Both GPUs can support UVA, enabling...\n",
    "Allocating buffers (64MB on GPU0, GPU1 and CPU Host)...\n",
    "Creating event handles...\n",
    "cudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 43.56GB/s\n",
    "Preparing host buffer and memcpy to GPU0...\n",
    "Run kernel on GPU1, taking source data from GPU0 and writing to GPU1...\n",
    "Run kernel on GPU0, taking source data from GPU1 and writing to GPU0...\n",
    "Copy data back to host from GPU0 and verify results...\n",
    "Disabling peer access...\n",
    "Shutting down...\n",
    "Test passed\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P2P bandwidth latency test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]\n",
    "Device: 0, GeForce RTX 2080 Ti, pciBusID: 3, pciDeviceID: 0, pciDomainID:0\n",
    "Device: 1, GeForce RTX 2080 Ti, pciBusID: 4, pciDeviceID: 0, pciDomainID:0\n",
    "Device=0 CAN Access Peer Device=1\n",
    "Device=1 CAN Access Peer Device=0\n",
    "\n",
    "***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\n",
    "So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.\n",
    "\n",
    "P2P Connectivity Matrix\n",
    "     D\\D     0     1\n",
    "     0\t     1     1\n",
    "     1\t     1     1\n",
    "Unidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1 \n",
    "     0 529.30   5.96 \n",
    "     1   5.97 531.94 \n",
    "Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\n",
    "   D\\D     0      1 \n",
    "     0 530.74  46.97 \n",
    "     1  46.97 532.19 \n",
    "Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1 \n",
    "     0 533.70   6.47 \n",
    "     1   6.47 529.23 \n",
    "Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1 \n",
    "     0 535.00  93.47 \n",
    "     1  93.51 525.33 \n",
    "P2P=Disabled Latency Matrix (us)\n",
    "   GPU     0      1 \n",
    "     0   1.99  13.08 \n",
    "     1  16.19   1.40 \n",
    "\n",
    "   CPU     0      1 \n",
    "     0   2.14   4.93 \n",
    "     1   4.78   2.15 \n",
    "P2P=Enabled Latency (P2P Writes) Matrix (us)\n",
    "   GPU     0      1 \n",
    "     0   1.98   2.05 \n",
    "     1   1.80   1.40 \n",
    "\n",
    "   CPU     0      1 \n",
    "     0   2.15   1.72 \n",
    "     1   1.71   2.17 \n",
    "\n",
    "NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional bandwidth with 2 2080 Ti over NVlink: ~94 GB/s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Tensorflow performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guide\n",
    "\n",
    "This is to show how to run a Tensorflow job with the existing container(s). If other frameworks are needed, simply pull the corresponding containers from Nvidia GPU Cloud: https://ngc.nvidia.com/catalog/containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download BigLSTM and ImageNet Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nvidia-docker run --rm -it -v /local_data_repository:/workspace/data nvcr.io/nvidia/tensorflow:19.01-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /workspace/nvidia-samples\n",
    "# There're samples of how to run big LSTM and ImageNet. Data downloading files included. It would be parsed\n",
    "# to local repository real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: data is stored in the sda disk (with 5.5T storage space), which can be found in /mnt/data. If reboot, please mount the sda disk first (see Section 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the data folder is mounted to **'/workspace/data'** in the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo nvidia-docker run --rm -it -v /mnt/data/:/workspace/data nvcr.io/nvidia/tensorflow:18.12-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we access the image 'nvcr.io/nvidia/tensorflow:18.12-py3' through a container.\n",
    "\n",
    "About -v: [Directory in the host system]:[Remote directory in the container]\n",
    "\n",
    "In this way the two directories will be synchronized. After existing from the container, the remote directory will be gone as well as everything in it, while the local directory remains in the host system. This is used for mounting data and the output log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we should see something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root@fd0901fc435e:/workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're in the container. Tensorflow is ready! Try the following command to load tensorflow and check the version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 -c 'import tensorflow as tf; print(tf.__version__)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Output: 1.12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nvidia examples are in '/workspace/nvidia-examples', including cnn, lstm, etc.\n",
    "\n",
    "After running scripts, leave the container by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test A: ResNet-50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /workspace/nvidia-examples/cnn\n",
    "\n",
    "mpiexec --allow-run-as-root --bind-to socket -np 2 python resnet.py --layers=50 --batch_size=64 --data_dir=/workspace/data/imagenet --precision=fp16 --log_dir=/workspace/data/imagenet/log-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size is set as 64, to avoid any memory problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-ce6c39274c89>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-ce6c39274c89>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ~~~\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Step Epoch Img/sec   Loss  LR\n",
    "    ...........\n",
    "589140  58.9   789.5  2.554  4.178 0.23933\n",
    "589150  58.9   790.9  3.261  4.884 0.23931\n",
    "589160  58.9   804.0  2.994  4.617 0.23930\n",
    "589170  58.9   798.2  3.155  4.779 0.23928\n",
    "589180  58.9   788.8  2.892  4.515 0.23927\n",
    "589190  58.9   796.0  3.109  4.732 0.23925\n",
    "589200  58.9   804.8  3.194  4.817 0.23924\n",
    "589210  58.9   798.4  2.964  4.588 0.23922\n",
    "589220  58.9   790.2  2.709  4.332 0.23921\n",
    "589230  58.9   788.3  2.962  4.585 0.23919\n",
    "589240  58.9   785.4  2.781  4.404 0.23918\n",
    "589250  58.9   793.3  3.134  4.758 0.23916\n",
    "589260  58.9   761.0  2.788  4.412 0.23914\n",
    "589270  58.9   794.4  3.416  5.040 0.23913\n",
    "589280  58.9   781.0  2.890  4.514 0.23911\n",
    "589290  58.9   812.4  2.891  4.515 0.23910\n",
    "589300  58.9   795.1  2.637  4.260 0.23908\n",
    "589310  58.9   797.8  3.128  4.752 0.23907\n",
    "589320  58.9   785.9  2.649  4.272 0.23905\n",
    "589330  58.9   794.2  2.632  4.256 0.23904\n",
    "589340  58.9   803.8  3.182  4.806 0.23902\n",
    "589350  58.9   775.7  2.484  4.108 0.23901\n",
    "589360  58.9   803.2  2.517  4.140 0.23899\n",
    "589370  58.9   790.5  3.282  4.906 0.23898\n",
    "589380  58.9   781.7  3.140  4.764 0.23896\n",
    "589390  58.9   777.6  3.054  4.678 0.23894\n",
    "589400  58.9   782.6  3.221  4.844 0.23893\n",
    "589410  58.9   785.9  3.371  4.995 0.23891\n",
    "589420  58.9   786.7  2.936  4.560 0.23890\n",
    "589430  58.9   795.8  3.240  4.864 0.23888\n",
    "589440  58.9   760.9  2.516  4.139 0.23887\n",
    "589450  58.9   768.1  3.277  4.901 0.23885\n",
    "589460  58.9   794.9  3.272  4.896 0.23884\n",
    "    ..........."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed** : ~800 images/sec\n",
    "\n",
    "Reference speed: 776 images/sec [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  Step Epoch Img/sec   Loss  LR\n",
    "    ...........\n",
    "91710   9.2   788.1  3.690  5.545 1.61347\n",
    "91720   9.2   820.9  3.733  5.588 1.61343\n",
    "91730   9.2   789.8  3.727  5.582 1.61339\n",
    "91740   9.2   806.5  3.421  5.276 1.61335\n",
    "91750   9.2   791.3  3.118  4.974 1.61331\n",
    "91760   9.2   792.8  3.453  5.309 1.61327\n",
    "91770   9.2   806.7  3.751  5.607 1.61323\n",
    "91780   9.2   805.9  3.047  4.904 1.61319\n",
    "91790   9.2   798.4  3.814  5.671 1.61315\n",
    "91800   9.2   784.6  3.643  5.500 1.61311\n",
    "91810   9.2   797.7  3.630  5.486 1.61307\n",
    "91820   9.2   804.7  3.616  5.473 1.61303\n",
    "91830   9.2   784.4  3.543  5.400 1.61299\n",
    "91840   9.2   794.3  3.498  5.355 1.61295\n",
    "91850   9.2   793.0  3.976  5.833 1.61291\n",
    "91860   9.2   810.0  3.082  4.939 1.61287\n",
    "91870   9.2   788.0  3.805  5.662 1.61283\n",
    "91880   9.2   805.9  3.732  5.589 1.61279\n",
    "91890   9.2   812.4  3.952  5.809 1.61275\n",
    "91900   9.2   809.6  3.513  5.371 1.61271\n",
    "91910   9.2   807.3  3.975  5.833 1.61267\n",
    "91920   9.2   794.6  3.808  5.665 1.61263\n",
    "91930   9.2   810.4  3.750  5.608 1.61259\n",
    "..........."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Speed:** ~800 images/sec\n",
    "\n",
    "Reference speed: 776 images/sec [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test B: Big LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /workspace/nvidia-examples/big_lstm\n",
    "# max_time=36000 (seconds)\n",
    "python single_lm_train.py --mode=train --logdir=/workspace/one-billion-word/log-dir --num_gpus=2 --datadir=/workspace/one-billion-word/1-billion-word-language-modeling-benchmark-r13output/ --hpconfig run_profiler=False,max_time=36000, num_steps=20,num_shards=8,num_layers=2,learning_rate=0.2,max_grad_norm=1,keep_prob=0.9,emb_size=1024,projected_size=1024,state_size=8192,num_sampled=8192,batch_size=256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** On 21st Jan, there's a system malfunction with beta after a rebooting. The UI couldn’t be launched. After a series of actions including upgrading the Nvidia driver, upgrading the packages, changing the display manager, re-installing the desktop manager, removing the nvidia-prime configuration, etc., the problem has been fixed. However, the performance of the big_lstm test with one-billion-word dataset on beta HAS CHANGED significantly. Please see below for more details about big_lstm performance before and after the system malfunction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "- Before 21st Jan,\n",
    "\n",
    "Speed: ~60,000 words/sec\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "- After 21st Jan,\n",
    "\n",
    "Speed: ~14,400 words/sec\n",
    "\n",
    "Reference speed: 16,977 words/sec [1]\n",
    "\n",
    "                 12,728 words/sec with batch_size = 512 on DGX1 (from /workspace/nvidia-examples/big_lstm/README.md)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Do not use batch_size = 512 for beta. The system would normally run out of memory. If worse, it would shut down because of overheating. Additionally, it shut down once for batch_size = 256 during the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "Speed: ~60,000 words/sec\n",
    "\n",
    "Reference speed: 16,977 words/sec [1]\n",
    "\n",
    "                 12,728 words/sec with batch_size = 512 on DGX1 (from /workspace/nvidia-examples/big_lstm/README.md)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Gamma can deal with batch_size = 512/1024 without major affections on the number of words per sec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] RTX 2080Ti with NVLINK - TensorFlow Performance (Includes Comparison with GTX 1080Ti, RTX 2070, 2080, 2080Ti and Titan V) [https://www.pugetsystems.com/labs/hpc/RTX-2080Ti-with-NVLINK---TensorFlow-Performance-Includes-Comparison-with-GTX-1080Ti-RTX-2070-2080-2080Ti-and-Titan-V-1267/#should-you-get-an-rtx-2080ti-or-two-or-more-for-machine-learning-work]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### SSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beta: 10.70.26.216\n",
    "\n",
    "gamma: 10.70.26.62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### TeamViewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TeamViewer 14 has been installed on beta and gamma. Two machines are sharing one monitor. Assume the monitor switch has been turned to beta, there could be a problem when accessing gamma through TeamViewer, and vice versa. The remote control window would freeze. This problem has been found on Ubuntu 18.04 with TeamViewer 14."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
